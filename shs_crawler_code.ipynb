{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium 網站爬蟲\n",
    "# 中學生網站 - 閱讀心得寫作比賽參賽作品\n",
    "\n",
    "- 主網頁: https://www.shs.edu.tw/\n",
    "- 搜尋頁: https://www.shs.edu.tw/index.php?p=search\n",
    "- 作品頁: https://www.shs.edu.tw/search_view_over.php?work_id=2432715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "__author__ = 'I-HSUAN WU'\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# browser-based web crawler\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"ignore-certificate-errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_content(browser, url):\n",
    "    print('Doing', url)\n",
    "    browser.get(url)          #執行url\n",
    "    time.sleep(randint(0, 1)) #停留時間，隨機取數0~1，因為太過頻繁會被封鎖\n",
    "    \n",
    "    try:\n",
    "        reward_xpath = '/html/body/div[1]/div[2]/div[2]/table[1]/tbody/tr/td'\n",
    "        reward = browser.find_element_by_xpath(reward_xpath).text.strip()\n",
    "\n",
    "        school_xpath = '/html/body/div[1]/div[2]/div[2]/table[3]/tbody/tr/td[2]'\n",
    "        school = browser.find_element_by_xpath(school_xpath).text.replace('\\u3000',' ').strip()\n",
    "\n",
    "        ####################\n",
    "        author_xpath = '/html/body/div[1]/div[2]/div[2]/table[4]/tbody/tr[1]/td[2]' # find the xpath\n",
    "        author = browser.find_element_by_xpath(author_xpath).text.strip()\n",
    "\n",
    "        ####################\n",
    "        title_xpath = '/html/body/div[1]/div[2]/div[2]/table[4]/tbody/tr[2]/td[2]' # find the xpath\n",
    "        title = browser.find_element_by_xpath(title_xpath).text.strip()\n",
    "\n",
    "        ####################\n",
    "        ISBN_xpath = '/html/body/div[1]/div[2]/div[2]/table[4]/tbody/tr[3]/td[2]' # find the xpath\n",
    "        ISBN = browser.find_element_by_xpath(ISBN_xpath).text.strip()\n",
    "\n",
    "        ####################\n",
    "        zh_book_xpath = '/html/body/div[1]/div[2]/div[2]/table[4]/tbody/tr[4]/td[2]' # find the xpath\n",
    "        zh_book = browser.find_element_by_xpath(zh_book_xpath).text.strip()\n",
    "\n",
    "        ####################\n",
    "        orig_book_xpath = '/html/body/div[1]/div[2]/div[2]/table[4]/tbody/tr[5]/td[2]' # find the xpath    \n",
    "        orig_book = browser.find_element_by_xpath(orig_book_xpath).text.strip()\n",
    "\n",
    "        ####################\n",
    "        book_author_xpath = '/html/body/div[1]/div[2]/div[2]/table[4]/tbody/tr[6]/td[2]' # find the xpath\n",
    "        book_author = browser.find_element_by_xpath(book_author_xpath).text.strip()\n",
    "\n",
    "        ####################\n",
    "        book_compiler_xpath = '/html/body/div[1]/div[2]/div[2]/table[4]/tbody/tr[7]/td[2]' # find the xpath\n",
    "        book_compiler = browser.find_element_by_xpath(book_compiler_xpath).text.strip()\n",
    "\n",
    "        ####################\n",
    "        publication_org_xpath = '/html/body/div[1]/div[2]/div[2]/table[4]/tbody/tr[8]/td[2]' # find the xpath\n",
    "        publication_org = browser.find_element_by_xpath(publication_org_xpath).text.strip()\n",
    "\n",
    "        ####################\n",
    "        publication_date_xpath = '/html/body/div[1]/div[2]/div[2]/table[4]/tbody/tr[9]/td[2]' # find the xpath\n",
    "        publication_date= browser.find_element_by_xpath(publication_date_xpath).text.strip()\n",
    "\n",
    "        ####################\n",
    "        revision_xpath = '/html/body/div[1]/div[2]/div[2]/table[4]/tbody/tr[10]/td[2]' # find the xpath\n",
    "        revision = browser.find_element_by_xpath(revision_xpath).text.strip()\n",
    "\n",
    "        ####################\n",
    "        my_view_xpath = '/html/body/div[1]/div[2]/div[2]/table[7]/tbody/tr[2]/td' # find the xpath\n",
    "        my_view = [line.strip() for line in browser.find_element_by_xpath(my_view_xpath).text.replace('<br>','').split('\\n') if line.strip() != '']\n",
    "\n",
    "        ####################\n",
    "        book_author_intr_xpath = '/html/body/div[1]/div[2]/div[2]/table[5]/tbody/tr[2]/td' # find the xpath\n",
    "        book_author_intr = [line.strip() for line in browser.find_element_by_xpath(book_author_intr_xpath).text.replace('<br>','').split('\\n') if line.strip() != '']\n",
    "\n",
    "        ####################\n",
    "        abstract_xpath = '/html/body/div[1]/div[2]/div[2]/table[6]/tbody/tr[2]/td' # find the xpath\n",
    "        abstract = [line.strip() for line in browser.find_element_by_xpath(abstract_xpath).text.replace('<br>','').split('\\n') if line.strip() != '']\n",
    "\n",
    "        ####################\n",
    "        issue_xpath = '/html/body/div[1]/div[2]/div[2]/table[8]/tbody/tr[2]/td' # find the xpath\n",
    "        issue = [line.strip() for line in browser.find_element_by_xpath(issue_xpath).text.replace('<br>','').split('\\n') if line.strip() != '']\n",
    "\n",
    "\n",
    "        return {'url':url,\n",
    "                '作品':reward, '學校名稱':school, '作者':author, \n",
    "            '參賽標題':title, '書籍ISBN':ISBN, '中文書名':zh_book, '原文書名':orig_book,\n",
    "            '書籍作者':book_author, '書籍編譯者':book_compiler, \n",
    "            '出版單位':publication_org, '出版年月':publication_date, '版次':revision,\n",
    "            '圖書作者與內容簡介':book_author_intr, '內容摘錄':abstract,\n",
    "            '我的觀點': my_view, '討論議題':issue}\n",
    "    except:\n",
    "        print('ERROR: ', url)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler_article(i, urls):\n",
    "\n",
    "    browser = webdriver.Chrome(executable_path=DRIVER_PATH, options=options)   #開視窗\n",
    "    articles = [article_content(browser, url) for url in urls]\n",
    "    articles = [art for art in articles if art is not None]\n",
    "    browser.close()  #關視窗\n",
    "\n",
    "    with open(os.path.join(ARTICLE_SAVE_PATH, 'result_article_'+str(i)+'.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(articles, f, ensure_ascii=False, indent=2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler_all_article(url_file_name, num_set):\n",
    "    with open(url_file_name, 'r', encoding='utf-8') as f:\n",
    "        href = [line for line in f.read().split('\\n') if line != '']\n",
    "    print('Number of URLs: %d'%(len(href)))\n",
    "\n",
    "    # split URLs into several sets\n",
    "    urls_list = np.array_split(np.array(href), num_set)\n",
    "    for i,urls in enumerate(tqdm(urls_list)):\n",
    "        if i < 20:\n",
    "            continue     #只爬第21,22個集合\n",
    "        crawler_article(i, urls)\n",
    "\n",
    "    print('All articles have DONE!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler_href(browser, la, lo):\n",
    "    print(la, lo)\n",
    "    browser.get(MAIN_URL)\n",
    "    s_contest_number = Select(browser.find_element_by_id('s_contest_number'))\n",
    "    s_contest_number.select_by_value(la)\n",
    "    s_area = Select(browser.find_element_by_id('s_area'))\n",
    "    s_area.select_by_visible_text(lo)\n",
    "    browser.find_element_by_id('search_button').click()\n",
    "\n",
    "    href = [elem.get_attribute('href') for elem in browser.find_elements_by_xpath(\"//a[@href]\")]\n",
    "    href = [h for h in href if h is not None and 'work_id=' in h]\n",
    "    return href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler_all_href(url_file_name):\n",
    "    browser = webdriver.Chrome(executable_path=DRIVER_PATH, options=options)\n",
    "    browser.get(MAIN_URL)\n",
    "    s_contest_number = browser.find_element_by_id('s_contest_number')\n",
    "    s_contest_number = s_contest_number.find_elements_by_tag_name('option')\n",
    "    \n",
    "    ####################\n",
    "    ladder = [item.text for item in s_contest_number][:1]  # first one\n",
    "    \n",
    "    s_area = browser.find_element_by_id('s_area') # find the id\n",
    "    s_area = s_area.find_elements_by_tag_name('option')\n",
    "    \n",
    "    ####################\n",
    "    location = [item.text for item in s_area if item.text != '--'][:1] # first two\n",
    "    \n",
    "    print('Number of pages: %d',len(location)*len(ladder))\n",
    "\n",
    "    results = [crawler_href(browser, i,j) for i in ladder for j in location] # 2114\n",
    "    browser.close()\n",
    "    with open(url_file_name, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join([j for i in results for j in i]))\n",
    "    print('All URLs have DONE!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: %d 1\n",
      "1090315 南投區\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All URLs have DONE!!!\n",
      "Number of URLs: 203\n",
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2373242\n",
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2448518\n",
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2364126\n",
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2432534\n",
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2433218\n",
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2437900\n",
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2448470\n",
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2445736\n",
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2444506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 21/22 [00:42<00:02,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2454097\n",
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2431766\n",
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2431756\n",
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2432957\n",
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2437888\n",
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2454122\n",
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2448436\n",
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2453112\n",
      "Doing https://www.shs.edu.tw/search_view_over.php?work_id=2458921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [01:04<00:00,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All articles have DONE!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#初始化啟動chrome webdriver\n",
    "DRIVER_PATH = \"./chromedriver\"\n",
    "MAIN_URL = 'https://www.shs.edu.tw/index.php?p=search'\n",
    "ARTICLE_SAVE_PATH = '/Users/yuni/Retrieval/selenium_data'\n",
    "URL_FILE_NAME = '/Users/yuni/Retrieval/selenium_data/url_list.txt'\n",
    "NUM_set = 22\n",
    "\n",
    "crawler_all_href(url_file_name=URL_FILE_NAME)\n",
    "crawler_all_article(url_file_name=URL_FILE_NAME, \n",
    "                        num_set=NUM_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
