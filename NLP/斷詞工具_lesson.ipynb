{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yuni/stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from stanza.utils.resources import DEFAULT_MODEL_DIR\n",
    "print(DEFAULT_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download English model\n",
    "# stanza.download('en')\n",
    "#Download traditional Chinese model\n",
    "# stanza.download('zh-hant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts \n",
    "en_doc = 'On April 3, the Central Epidemic Command Center (CECC) reported that that 827 additional cases related to coronavirus disease 2019 (COVID-19) were reported on April 2.'\n",
    "zh_doc = '中央流行疫情指揮中心今(3)日表示，昨(2)日國內新增827例新型冠狀病毒肺炎相關通報， 截至目前累計34,557例(含30,530例排除)，其中348例確診(今日新增案340至348)，分別為 298例境外移入及50例本土病例。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-12 20:53:45 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "| lemma     | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-04-12 20:53:45 INFO: Use device: cpu\n",
      "2020-04-12 20:53:45 INFO: Loading: tokenize\n",
      "2020-04-12 20:53:45 INFO: Loading: pos\n",
      "2020-04-12 20:53:46 INFO: Loading: lemma\n",
      "2020-04-12 20:53:46 INFO: Done loading processors!\n",
      "2020-04-12 20:53:46 INFO: Loading these models for language: zh-hant (Traditional_Chinese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "=======================\n",
      "\n",
      "2020-04-12 20:53:46 INFO: Use device: cpu\n",
      "2020-04-12 20:53:46 INFO: Loading: tokenize\n",
      "2020-04-12 20:53:46 INFO: Loading: pos\n",
      "2020-04-12 20:53:48 INFO: Loading: lemma\n",
      "2020-04-12 20:53:48 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# initialize English neural pipeline\n",
    "en_nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma,pos', use_gpu=False)\n",
    "# initialize Chinese neural pipeline\n",
    "zh_nlp = stanza.Pipeline(lang='zh-hant', processors='tokenize,lemma,pos', use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run annotation over a English sentence\n",
    "en_annotated_doc = en_nlp(en_doc)\n",
    "#run annotation over a Chinese sentence\n",
    "zh_annotated_doc = zh_nlp(zh_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['中央', '流行', '疫情', '指揮', '中心', '今(3)', '日', '表示', '，', '昨(', '2)', '日國內', '新增', '827', '例', '新型', '冠狀', '病毒', '肺炎', '相關', '通報', '，', '截至', '目前', '累計', '34,557', '例(含30,530', '例排除', ')', '，', '其中', '348', '例確診(', '今', '日新', '增案', '340', '至', '348)', '，', '分別', '為', '298', '例', '境外', '移入', '及', '50', '例', '本土', '病例', '。']\n"
     ]
    }
   ],
   "source": [
    "fin = []\n",
    "for i, sent in enumerate(zh_annotated_doc.sentences):\n",
    "    for j, word in enumerate(sent.words):\n",
    "        fin.append(format(word.text))\n",
    "print(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English annotation\n",
      "Sentence {}\n",
      "INDEX\tTEXT\tLEMMA\tPOS 1\n",
      "0\tOn\ton\tADP\n",
      "1\tApril\tApril\tPROPN\n",
      "2\t3\t3\tNUM\n",
      "3\t,\t,\tPUNCT\n",
      "4\tthe\tthe\tDET\n",
      "5\tCentral\tCentral\tPROPN\n",
      "6\tEpidemic\tEpidemic\tPROPN\n",
      "7\tCommand\tCommand\tPROPN\n",
      "8\tCenter\tCenter\tPROPN\n",
      "9\t(\t(\tPUNCT\n",
      "10\tCECC\tCECC\tPROPN\n",
      "11\t)\t)\tPUNCT\n",
      "12\treported\treport\tVERB\n",
      "13\tthat\tthat\tSCONJ\n",
      "14\tthat\tthat\tSCONJ\n",
      "15\t827\t827\tNUM\n",
      "16\tadditional\tadditional\tADJ\n",
      "17\tcases\tcase\tNOUN\n",
      "18\trelated\trelated\tADJ\n",
      "19\tto\tto\tADP\n",
      "20\tcoronavirus\tcoronavirus\tNOUN\n",
      "21\tdisease\tdisease\tNOUN\n",
      "22\t2019\t2019\tNUM\n",
      "23\t(\t(\tPUNCT\n",
      "24\tCOVID\tCOVID\tPROPN\n",
      "25\t-\t-\tSYM\n",
      "26\t19\t19\tNUM\n",
      "27\t)\t)\tPUNCT\n",
      "28\twere\tbe\tAUX\n",
      "29\treported\treport\tVERB\n",
      "30\ton\ton\tADP\n",
      "31\tApril\tApril\tPROPN\n",
      "32\t2\t2\tNUM\n",
      "33\t.\t.\tPUNCT\n",
      "Chinese annotation\n",
      "Sentence {}\n",
      "INDEX\tTEXT\tLEMMA\tPOS 1\n",
      "0\t中央\t中央\tNOUN\n",
      "1\t流行\t流行\tADJ\n",
      "2\t疫情\t疫情\tNOUN\n",
      "3\t指揮\t指揮\tNOUN\n",
      "4\t中心\t中心\tNOUN\n",
      "5\t今(3)\t今(3)\tX\n",
      "6\t日\t日\tNOUN\n",
      "7\t表示\t表示\tVERB\n",
      "8\t，\t，\tPUNCT\n",
      "9\t昨(\t昨(\tNOUN\n",
      "10\t2)\t2)\tNUM\n",
      "11\t日國內\t日國內\tNOUN\n",
      "12\t新增\t新增\tVERB\n",
      "13\t827\t827\tNUM\n",
      "14\t例\t例\tNOUN\n",
      "15\t新型\t新型\tNOUN\n",
      "16\t冠狀\t冠狀\tNOUN\n",
      "17\t病毒\t病毒\tNOUN\n",
      "18\t肺炎\t肺炎\tNOUN\n",
      "19\t相關\t相關\tADJ\n",
      "20\t通報\t通報\tNOUN\n",
      "21\t，\t，\tPUNCT\n",
      "22\t截至\t截至\tADP\n",
      "23\t目前\t目前\tNOUN\n",
      "24\t累計\t累計\tVERB\n",
      "25\t34,557\t34,557\tNUM\n",
      "26\t例(含30,530\t例(含30,530\tNUM\n",
      "27\t例排除\t例排除\tNOUN\n",
      "28\t)\t)\tPART\n",
      "29\t，\t，\tPUNCT\n",
      "30\t其中\t其中\tNOUN\n",
      "31\t348\t348\tNUM\n",
      "32\t例確診(\t例確診(\tNOUN\n",
      "33\t今\t今\tNOUN\n",
      "34\t日新\t日新\tPROPN\n",
      "35\t增案\t增案\tVERB\n",
      "36\t340\t340\tNUM\n",
      "37\t至\t至\tCCONJ\n",
      "38\t348)\t348)\tNUM\n",
      "39\t，\t，\tPUNCT\n",
      "40\t分別\t分別\tADV\n",
      "41\t為\t為\tAUX\n",
      "42\t298\t298\tNUM\n",
      "43\t例\t例\tNOUN\n",
      "44\t境外\t境外\tNOUN\n",
      "45\t移入\t移入\tVERB\n",
      "46\t及\t及\tCCONJ\n",
      "47\t50\t50\tNUM\n",
      "48\t例\t例\tNOUN\n",
      "49\t本土\t本土\tNOUN\n",
      "50\t病例\t病例\tNOUN\n",
      "51\t。\t。\tPUNCT\n"
     ]
    }
   ],
   "source": [
    "def show(doc):\n",
    "    for i, sent in enumerate(doc.sentences):\n",
    "        print(\"Sentence {}\\nINDEX\\tTEXT\\tLEMMA\\tPOS\",format(i+1))\n",
    "        for j, word in enumerate(sent.words):\n",
    "            print (\"{}\\t{}\\t{}\\t{}\".format(j,word.text,word.lemma,word.pos))\n",
    "            \n",
    "        #print(\"\")\n",
    "        \n",
    "print('English annotation')\n",
    "show(en_annotated_doc)\n",
    "print('Chinese annotation')\n",
    "show(zh_annotated_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ckip Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from ckiptagger import data_utils\n",
    "# path = os.path.join(str(Path.home()), 'ckip/')\n",
    "# if not os.path.exists(path): os.mkdir(path)\n",
    "# data_utils.download_data_gdown(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download traditional Chinese model\n",
    "path = str(Path.home())+'/ckip'\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    data_utils.download_data_gdown(path) # gdrive-ckip 2GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts \n",
    "zh_doc = '中央流行疫情指揮中心今(3)日表示，昨(2)日國內新增827例新型冠狀病毒肺炎相關通報， 截至目前累計34,557例(含30,530例排除)，其中348例確診(今日新增案340至348)，分別為 298例境外移入及50例本土病例。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Chinese neural pipeline\n",
    "zh_ws = WS(path+\"/data\")\n",
    "zh_pos = POS(path+\"/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run annotation over a Chinese sentence\n",
    "zh_annotated_ws = zh_ws([zh_doc])\n",
    "zh_annotated_pos = zh_pos(zh_annotated_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese annotation\n",
      "sentence 0:\n",
      "INDEX\tTEXT\tPOS\n",
      "0\t中央\tNc\n",
      "1\t流行\tVH\n",
      "2\t疫情\tNa\n",
      "3\t指揮\tVC\n",
      "4\t中心\tNc\n",
      "5\t今\tNd\n",
      "6\t(3)\tNeu\n",
      "7\t日\tNd\n",
      "8\t表示\tVE\n",
      "9\t，\tCOMMACATEGORY\n",
      "10\t昨\tNd\n",
      "11\t(2)\tNeu\n",
      "12\t日\tNd\n",
      "13\t國內\tNc\n",
      "14\t新增\tVJ\n",
      "15\t827\tNeu\n",
      "16\t例\tNa\n",
      "17\t新型\tNa\n",
      "18\t冠狀\tNa\n",
      "19\t病毒\tNa\n",
      "20\t肺炎\tNa\n",
      "21\t相關\tVH\n",
      "22\t通報\tVE\n",
      "23\t，\tCOMMACATEGORY\n",
      "24\t \tWHITESPACE\n",
      "25\t截至\tP\n",
      "26\t目前\tNd\n",
      "27\t累計\tVJ\n",
      "28\t34,557\tNeu\n",
      "29\t例\tNa\n",
      "30\t(\tPARENTHESISCATEGORY\n",
      "31\t含\tVJ\n",
      "32\t30,530\tNeu\n",
      "33\t例\tNa\n",
      "34\t排除\tVC\n",
      "35\t)\tPARENTHESISCATEGORY\n",
      "36\t，\tCOMMACATEGORY\n",
      "37\t其中\tNep\n",
      "38\t348\tNeu\n",
      "39\t例\tNa\n",
      "40\t確診\tVA\n",
      "41\t(\tPARENTHESISCATEGORY\n",
      "42\t今日\tNd\n",
      "43\t新增案\tNa\n",
      "44\t340\tNeu\n",
      "45\t至\tCaa\n",
      "46\t348\tNeu\n",
      "47\t)\tPARENTHESISCATEGORY\n",
      "48\t，\tCOMMACATEGORY\n",
      "49\t分別\tD\n",
      "50\t為\tVG\n",
      "51\t 298\tFW\n",
      "52\t例\tNa\n",
      "53\t境\tNa\n",
      "54\t外\tNcd\n",
      "55\t移入\tVC\n",
      "56\t及\tCaa\n",
      "57\t50\tNeu\n",
      "58\t例\tNa\n",
      "59\t本土\tNc\n",
      "60\t病例\tNa\n",
      "61\t。\tPERIODCATEGORY\n"
     ]
    }
   ],
   "source": [
    "print('Chinese annotation')\n",
    "for i, sentence in enumerate(zip(zh_annotated_ws,zh_annotated_pos)):\n",
    "    print('sentence {}:' .format(i))\n",
    "    print('INDEX\\tTEXT\\tPOS')\n",
    "    for j,word in enumerate(zip(sentence[0], sentence[1])):\n",
    "        print('{}\\t{}\\t{}'.format(j, word[0], word[1]))\n",
    "        #print(list(word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese annotation\n",
      "['中央', '流行', '疫情', '指揮', '中心', '今', '(3)', '日', '表示', '，', '昨', '(2)', '日', '國內', '新增', '827', '例', '新型', '冠狀', '病毒', '肺炎', '相關', '通報', '，', ' ', '截至', '目前', '累計', '34,557', '例', '(', '含', '30,530', '例', '排除', ')', '，', '其中', '348', '例', '確診', '(', '今日', '新增案', '340', '至', '348', ')', '，', '分別', '為', ' 298', '例', '境', '外', '移入', '及', '50', '例', '本土', '病例', '。']\n"
     ]
    }
   ],
   "source": [
    "print('Chinese annotation')\n",
    "res=[]\n",
    "for i, sentence in enumerate(zip(zh_annotated_ws,zh_annotated_pos)):\n",
    "    for j,word in enumerate(zip(sentence[0], sentence[1])):\n",
    "        res.append(format(word[0]))\n",
    "        #print(list(word[0]))\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yuni/cts_0301_0403_news.json\", 'r', encoding='utf-8') as f:\n",
    "    print(type(f))\n",
    "    res = json.load(f)\n",
    "    #讀取json大量資料錯誤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/yuni/cts_0301_0403_news.json\", 'r', encoding='utf-8') as f:\n",
    "    line = f.readline()\n",
    "    print(type(line))\n",
    "    res = json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"/Users/yuni/cts_0301_0403_news.json\", 'r', encoding='utf-8')\n",
    "papers = []\n",
    "for line in file.readlines():\n",
    "    dic = json.loads(line)\n",
    "    papers.append(dic)\n",
    "    \n",
    "data_list = []\n",
    "for i in papers:\n",
    "    data_list.append(i[\"title\"])\n",
    "    data_list.append(i[\"content\"])\n",
    "    \n",
    "data = str()\n",
    "for i in data_list:\n",
    "    data = str(data) + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import os\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downLoad traditional Chinese dictionary\n",
    "URL = 'https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big'\n",
    "if not os.path.exists(os .path.join(os.getcwd(), 'dict.txt.big')):\n",
    "    urllib.request.urlretrieve(URL, 'dict.txt.big')\n",
    "#jieba. set_dictionary( 'dict.txt.big')\n",
    "#jieba. Load_userdict( 'userdict.txt')\n",
    "#jieba.add_word('國内')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run annotation over a Chinese sentence\n",
    "# with POS tag\n",
    "zh_annotated_pos = list(pseg.cut(zh_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('INDEXItTEXTItPOS')\n",
    "for i,(word,pos) in enumerate(zh_annotated_pos):\n",
    "    print('{}\\t{}\\t{}'.format(i, word, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts \n",
    "zh_doc = '中央流行疫情指揮中心今(3)日表示，昨(2)日國內新增827例新型冠狀病毒肺炎相關通報， 截至目前累計34,557例(含30,530例排除)，其中348例確診(今日新增案340至348)，分別為 298例境外移入及50例本土病例。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jieba' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-318ddd5cd632>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# run annotation over a Chinese sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# accurate pattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mzh_annotated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjieba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzh_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'jieba' is not defined"
     ]
    }
   ],
   "source": [
    "# run annotation over a Chinese sentence\n",
    "# accurate pattern\n",
    "zh_annotated = list(jieba.cut(zh_doc, cut_all=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese annotation\n",
      "INDEXItTEXT\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'zh_annotated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-56440f4e2f6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Chinese annotation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'INDEXItTEXT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzh_annotated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}\\t{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'zh_annotated' is not defined"
     ]
    }
   ],
   "source": [
    "print('Chinese annotation')\n",
    "print( 'INDEXItTEXT')\n",
    "for j,word in enumerate(zh_annotated):\n",
    "    print('{}\\t{}'.format(j, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
