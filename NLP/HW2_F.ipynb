{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = json.load(open('cts_0301_0403_news.json', 'r', encoding='utf-8'))\n",
    "file = open(\"cts_0301_0403_news.json\", 'r', encoding='utf-8')\n",
    "papers = []\n",
    "for line in file.readlines():\n",
    "    dic = json.loads(line)\n",
    "    papers.append(dic)\n",
    "type(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初步斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for i in papers:\n",
    "    data_list.append(i[\"title\"])\n",
    "    data_list.append(i[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = str()\n",
    "for i in data_list:\n",
    "    data = str(data) + str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data_list))\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from stanza.utils.resources import DEFAULT_MODEL_DIR\n",
    "print(DEFAULT_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-12 07:16:21 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "| lemma     | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-04-12 07:16:21 INFO: Use device: cpu\n",
      "2020-04-12 07:16:21 INFO: Loading: tokenize\n",
      "2020-04-12 07:16:21 INFO: Loading: pos\n",
      "2020-04-12 07:16:22 INFO: Loading: lemma\n",
      "2020-04-12 07:16:22 INFO: Done loading processors!\n",
      "2020-04-12 07:16:22 INFO: Loading these models for language: zh-hant (Traditional_Chinese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "=======================\n",
      "\n",
      "2020-04-12 07:16:22 INFO: Use device: cpu\n",
      "2020-04-12 07:16:22 INFO: Loading: tokenize\n",
      "2020-04-12 07:16:22 INFO: Loading: pos\n",
      "2020-04-12 07:16:24 INFO: Loading: lemma\n",
      "2020-04-12 07:16:24 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "en_stanza_nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma,pos', use_gpu=False)\n",
    "zh_stanza_nlp = stanza.Pipeline(lang='zh-hant', processors='tokenize,lemma,pos', use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_annotated_stanza_doc = zh_stanza_nlp(data)#stanza最慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza = []\n",
    "for i, sent in enumerate(zh_annotated_stanza_doc.sentences):\n",
    "    for j, word in enumerate(sent.words):\n",
    "        stanza.append(format(word.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ckiptagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(Path.home())+'/ckip/'\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    data_utils.download_data_gdown(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000291B8223788>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000291B8223788>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000291B7BFB848>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000291B7BFB848>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000291B946FB48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000291B946FB48>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000291B94CEB88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000291B94CEB88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000291B5B99A08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000291B5B99A08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000291BF25DF88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000291BF25DF88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000291BD2D4748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000291BD2D4748>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000291CB3B43C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000291CB3B43C8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000291CB414E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x00000291CB414E48>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000291CB337E88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000291CB337E88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "zh_ws = WS(path+'/data/')\n",
    "zh_pos = POS(path+'/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_annotated_ws = zh_ws(\n",
    "    data_list,\n",
    "    # sentence_segmentation = True, # To consider delimiters\n",
    "    # segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\"}), # This is the defualt set of delimiters\n",
    "    # recommend_dictionary = dictionary # words in this dictionary are encouraged\n",
    "    # coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckip = []\n",
    "for i, sentence in enumerate(zip(zh_annotated_ws)):\n",
    "    for j, word in enumerate(zip(sentence[0])):\n",
    "        ckip.append(word[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from ckiptagger import WS,POS,NER\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from sklearn.metrics.classification import accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "URL = \"http://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big\"\n",
    "if not os.path.exists(os.path.join(os.getcwd(),\"dict.txt.big\")):\n",
    "    urllib.request.urlretrieve(URL,\"dict.txt.big\")\n",
    "jieba.set_dictionary(\"dict.txt.big\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from D:\\jupyter\\dict.txt.big ...\n",
      "2020-04-11 21:30:25 DEBUG: Building prefix dict from D:\\jupyter\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\user\\AppData\\Local\\Temp\\jieba.uc7e69de6020c035eaa31e022ef353dfe.cache\n",
      "2020-04-11 21:30:25 DEBUG: Loading model from cache C:\\Users\\user\\AppData\\Local\\Temp\\jieba.uc7e69de6020c035eaa31e022ef353dfe.cache\n",
      "Loading model cost 1.456 seconds.\n",
      "2020-04-11 21:30:26 DEBUG: Loading model cost 1.456 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2020-04-11 21:30:26 DEBUG: Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "jieba.set_dictionary('dict.txt.big')\n",
    "jieba.load_userdict(\"my.dict.txt\")\n",
    "print(DEFAULT_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_annotated_jieba = list(jieba.cut(data, cut_all=False))#jieba跑好快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1509058\n"
     ]
    }
   ],
   "source": [
    "jieba = []\n",
    "for j, word in enumerate(zh_annotated_jieba):\n",
    "    jieba.append(word)\n",
    "print(len(jieba))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1401926\n",
      "1351120\n",
      "1509058\n",
      "31470\n",
      "15487\n"
     ]
    }
   ],
   "source": [
    "merge_st_ck = list(set(stanza) | set(ckip))#stanza or ckip\n",
    "st_without_ck = list(set(stanza) - set(ckip))#stanza - ckip\n",
    "ck_without_st = list(set(ckip) - set(stanza))#ckip - stanza\n",
    "ji_without_st = list(set(jieba) - set(stanza))#jieba - stanza\n",
    "ji_without_ck = list(set(jieba) - set(ckip))#jieba - ckip\n",
    "dic_or = list(set(ji_without_st) | set(ji_without_ck))\n",
    "a = list(set(ji_without_ck) - set(ji_without_st))\n",
    "b = list(set(ji_without_st) - set(ji_without_ck))\n",
    "dic_and = list(set(dic_or) - set(a) - set(b))\n",
    "print(len(stanza))\n",
    "print(len(ckip))\n",
    "print(len(jieba))\n",
    "print(len(dic_or))#聯集\n",
    "print(len(dic_and))#交集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 匯出list\n",
    "with open(\"dic.txt\", 'w', encoding=\"utf-8\") as f:\n",
    "    for i in dic_and:\n",
    "        f.write(i)\n",
    "        f.write(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將15487筆斷詞結果匯出，人工篩選出3361筆，其中多包含政府機關名稱、成語，另外我們再加了台灣前百Youtuber的名單進去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3361"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 匯入\n",
    "import sys\n",
    "result = []\n",
    "with open('06170114_userdefine.txt','r') as f:\n",
    "    for line in f:\n",
    "        item = line.strip('\\n').split(',')\n",
    "        result.append(item)\n",
    "FINAL_dic = result[0]\n",
    "len(FINAL_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 優化斷詞結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_stanza_segment = []\n",
    "for i in range(len(papers)):\n",
    "    zh_annotated_stanza_doc = zh_stanza_nlp(papers[i][\"title\"])\n",
    "    stanza = []\n",
    "    for i, sent in enumerate(zh_annotated_stanza_doc.sentences):\n",
    "        for j, word in enumerate(sent.words):\n",
    "            stanza.append(format(word.text))\n",
    "    title_stanza_segment.append(stanza)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(title_stanza_segment)):    \n",
    "    papers[i][\"title_stanza_segment\"] = title_stanza_segment[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_stanza_segment = []\n",
    "for i in range(len(papers)):\n",
    "    zh_annotated_stanza_doc = zh_stanza_nlp(papers[i][\"content\"])\n",
    "    stanza = []\n",
    "    for i, sent in enumerate(zh_annotated_stanza_doc.sentences):\n",
    "        for j, word in enumerate(sent.words):\n",
    "            stanza.append(format(word.text))\n",
    "    content_stanza_segment.append(stanza) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(content_stanza_segment)):    \n",
    "    papers[i][\"content_stanza_segment\"] = content_stanza_segment[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ckiptagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list to dict\n",
    "word_to_weight = { i : 1 for i in FINAL_dic }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = construct_dictionary(word_to_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(papers)):\n",
    "    zh_annotated_ws = zh_ws([papers[i][\"title\"]], recommend_dictionary = dictionary)\n",
    "    dic={\"title_ckiptagger_segment\" : zh_annotated_ws[0]}\n",
    "    papers[i].update(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(papers)):\n",
    "    zh_annotated_ws = zh_ws([papers[i][\"content\"]], recommend_dictionary = dictionary)\n",
    "    dic={\"content_ckiptagger_segment\" : zh_annotated_ws[0]}\n",
    "    papers[i].update(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting a list of dictionaries to json\n",
    "with open('06170114_cts_0301_0403_news.json', 'w') as fout:\n",
    "    json.dump(backup_papers , fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
